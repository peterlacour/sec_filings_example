{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Q SEC Filing Sentiment and Similiarity Metrics Example Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from tqdm.notebook import tqdm\n",
    "from finpie import historical_prices\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# set plot styles\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['text.color'] = 'black'\n",
    "plt.rcParams[ \"figure.figsize\"] = (20, 10)\n",
    "\n",
    "# load custom classes\n",
    "from classes.sec_class import SecData\n",
    "from classes.wordcloud_class import wordCloud\n",
    "from classes.text_processing_class import textPreProcess\n",
    "\n",
    "sec = SecData()\n",
    "wc = wordCloud()\n",
    "tpp = textPreProcess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- add 10-Ks to fill the gap\n",
    "- cleaning:\n",
    "    - clean docs of 'Table of Contents'\n",
    "    - clean docs of Managementâ€™s Discussion and Analysis of Financial Condition and Results of Operations \n",
    "    - clean docs of forward looking statement or other common texts\n",
    "    - clean docs of table description such as: ''the following table shows'\n",
    "    - clean docs of table footnotes '(1)'\n",
    "    - clean other things and improve regex.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 10-Q SEC Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_dict = { 'AAPL': '0000320193' } # 'XOM': '0000034088', 'TSLA': '0001318605', 'JNJ': '0000200406' }\n",
    "ten_qs = {}\n",
    "for ticker, cik in tqdm(cik_dict.items()):\n",
    "    ten_qs[ticker] = sec.get_10qs( cik )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticker_dict = {}\n",
    "for ticker in cik_dict.keys():\n",
    "    print(ticker)\n",
    "    mdna = {}\n",
    "    for file_date, doc in tqdm(ten_qs[ticker].items()):\n",
    "        mdna[file_date] = sec.get_mdna(doc)\n",
    "    df = pd.DataFrame(mdna, index = ['MDnA']).transpose()\n",
    "    ticker_dict[ticker] = df\n",
    "    #display(df.head())\n",
    "# example\n",
    "print(ticker, ' example')\n",
    "display(df.head())\n",
    "print(df.iloc[0].values[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrative Example:\n",
    "ticker = 'AAPL'\n",
    "df = ticker_dict[ticker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classes\n",
    "wc = wordCloud()\n",
    "tpp = textPreProcess()\n",
    "tpp.textColumn = 'MDnA'\n",
    "\n",
    "# load stopwords\n",
    "stopword_files = glob.glob('./stopwords/*.txt')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "for file in stopword_files:\n",
    "    stopwords += list(pd.read_csv(file).iloc[:,0])\n",
    "stopwords = [ word.lower() for word in stopwords if type(word) == type(\"\") ]\n",
    "tpp.stopwords = stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split dataframe\n",
    "dflist = np.array_split(df,  mp.cpu_count() )\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    dfs = list(tqdm(pool.imap( tpp.lemmatize,  [ d for d in dflist ] ), total = len(dflist) ))\n",
    "df = pd.concat( dfs )\n",
    "\n",
    "wordCloudDict = { f'{ticker}: 10-Q Management\\'s Discussion and Analysis': df.lemmatised_text }\n",
    "wc.create_word_cloud( wordCloudDict,  masks = f'./logos/{ticker.lower()}_logo', title = \"company\", columns = 1, rows = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexica based sentiment example using Loughran McDonald Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "lmcd_dictionary = pd.read_csv('LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "columns = ['Word', 'Negative', 'Positive', 'Uncertainty', 'Litigious', 'Constraining', 'Superfluous', 'Interesting']\n",
    "lmcd_dictionary = lmcd_dictionary[columns]\n",
    "lmcd_dictionary.dropna(subset = ['Word'], inplace = True, axis = 0)\n",
    "lmcd_dictionary.reset_index(drop = True, inplace = True)\n",
    "\n",
    "def lmcd_sentiment(df_idx):\n",
    "    df_idx = df_idx.copy()\n",
    "    text = df_idx.text.values[0] #' '.join( literal_eval( df_idx.lemmatised_text.values[0] ) )\n",
    "    for i, word in enumerate(lmcd_dictionary.Word):\n",
    "        if word in text.upper():\n",
    "            df_idx[columns[1:]] += lmcd_dictionary.loc[i, columns[1:]].values\n",
    "    return pd.DataFrame(df_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns[1:]:\n",
    "    df[col] = 0\n",
    "# concatenated texts...\n",
    "df['text'] = [ ' '.join( literal_eval( txt ) ) for txt in df.lemmatised_text ]\n",
    "\n",
    "# calculate lexica based sentiment\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    dfs = list(tqdm(pool.imap( lmcd_sentiment, [d for d in np.array_split(df, len(df))] ), total = len(df) ))\n",
    "df = pd.concat( dfs )\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.sort_index(inplace = True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot absolute \n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.sort_index(inplace = True)\n",
    "df[columns[1:]].plot()\n",
    "plt.title(f'Absolute sentiment scores of {ticker}\\'s 10-Q filings', fontsize = 24)\n",
    "plt.show()\n",
    "\n",
    "# Get price data\n",
    "prices = historical_prices(ticker)\n",
    "prices['adj_close'][df.index[0]:].plot()\n",
    "plt.title(f'{ticker} stock price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quarter on quarter returns to dataframe\n",
    "# should really start at next days opening price but keeping this for simplicity for now\n",
    "df['quarter_returns'] = prices['adj_close'].loc[df.index].pct_change().shift(1)\n",
    "\n",
    "# rescale sentiment based on doc length -> \"average sentiments\"\n",
    "df['doc_length'] = [ len(d) for d in df.lemmatised_text ]\n",
    "for col in columns[1:]:\n",
    "    df[col] = df[col] / df['doc_length']\n",
    "\n",
    "# Show standardised sentiment\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.sort_index(inplace = True)\n",
    "df[columns[1:]].plot()\n",
    "plt.title(f'Sentiment scores of {ticker}\\'s 10-Q filings', fontsize = 24)\n",
    "plt.show()\n",
    "\n",
    "# correlation between returns and average metrics\n",
    "print('Correlation between returns and standardised metrics:')\n",
    "display(df[columns[1:] + ['quarter_returns']].corr())\n",
    "#sns.heatmap(df[columns[1:] + ['quarter_returns']].corr(), cmap = 'Blues')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculating % change of metric from report to report\n",
    "# superfluous has many zero values, still need to deal with this issue in percentage calculation,\n",
    "# and for other companies other metrics might have missing values\n",
    "df[columns[1:]] = df[columns[1:]].pct_change()\n",
    "\n",
    "# correlation between percentage changes and returns\n",
    "display(df[columns[1:] + ['quarter_returns']].corr())\n",
    "plt.title('Heatmap of correlations between sentiment percentage changes and quarterly returns', fontsize = 24)\n",
    "sns.heatmap(df[columns[1:] + ['quarter_returns']].corr(), cmap = 'Blues' )\n",
    "plt.show()\n",
    "df.to_csv('xom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Scores of 10-Q's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity with tfidf or word2vec\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_df = tfidf.fit_transform(df.text).toarray()\n",
    "rows = []\n",
    "for j in range(len(tfidf_df)):\n",
    "    columns = []\n",
    "    for i in range(len(tfidf_df)):\n",
    "        columns.append( cosine_similarity(tfidf_df[j].reshape(1, -1), tfidf_df[i].reshape(1, -1))[0][0] )\n",
    "    rows.append(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosine_similiarity = pd.DataFrame(rows, columns = df.index, index = df.index )\n",
    "display(cosine_similiarity)\n",
    "\n",
    "# similiarity heatmap plot\n",
    "sns.heatmap(cosine_similiarity, cmap = 'Blues')\n",
    "plt.title('Cosine Similiarity of 10-Q\\'s based on TFIDF', fontsize = 24 )\n",
    "plt.show()\n",
    "\n",
    "# plot of similiarity of previous reports to most recent report\n",
    "plt.plot(cosine_similiarity.iloc[:,-1])\n",
    "plt.title('Cosine similarity of most recent 10-Q with previous 10-Qs', fontsize = 24 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containment N-gram scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# containment function\n",
    "def calculate_containment(df1, df2, ngram_size):\n",
    "    text1 = df1.text\n",
    "    text2 = df2.text\n",
    "    counts = CountVectorizer(analyzer='word', ngram_range=(ngram_size, ngram_size))\n",
    "    ngrams = counts.fit_transform([text1, text2])\n",
    "    \n",
    "    ngram_array = ngrams.toarray()\n",
    "    intersect = np.amin(ngram_array, axis=0)\n",
    "    common_ngrams = sum(intersect)\n",
    "    \n",
    "    len_ngram_a = sum(ngram_array[0])\n",
    "    \n",
    "    containment_score = 1.0 * common_ngrams / len_ngram_a\n",
    "    \n",
    "    return containment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-ngram containment\n",
    "ngram = 2\n",
    "rows = []\n",
    "for j in range(len(df)):\n",
    "    columns = []\n",
    "    for i in range(len(df)):\n",
    "        print(i)\n",
    "        columns.append( calculate_containment( df.iloc[i], df.iloc[j], ngram ) )\n",
    "    rows.append(columns)\n",
    "containment_df = pd.DataFrame(rows, index = pd.to_datetime(df.index), columns = df.index )\n",
    "\n",
    "# containment df\n",
    "display(containment_df)\n",
    "\n",
    "# containment heatmap\n",
    "plt.title('Heatmap of 2-nrgam containment scores')\n",
    "sns.heatmap(containment_df, cmap = 'Blues')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot of containment of previous reports to most recent report\n",
    "plt.plot(containment_df.iloc[:,-1])\n",
    "plt.title('2-gram containment of most recent 10-Q with previous 10-Qs', fontsize = 24 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
